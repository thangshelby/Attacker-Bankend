"""
RAG Chatbot with Function Calling
Combines document search (RAG) with database function calling via MCP server
"""
import os
import time
import asyncio
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# Import existing components
from app.botagent.vectordb import PineconeManager
from app.botagent.mcp_server import StudentDataMCPServer

# LlamaIndex imports with fallback
try:
    from llama_index.core.chat_engine import CondensePlusContextChatEngine
    from llama_index.core.memory import ChatMemoryBuffer
    from llama_index.core import get_response_synthesizer
    from llama_index.core.retrievers import VectorIndexRetriever
    from llama_index.core.query_engine import RetrieverQueryEngine
    from llama_index.core.postprocessor import SimilarityPostprocessor
except ImportError as e:
    print(f"‚ùå LlamaIndex import error: {e}")
    print("   Install: pip install llama-index")

try:
    from llama_index.llms.openai import OpenAI
except ImportError:
    try:
        from llama_index_llms_openai import OpenAI
    except ImportError:
        print("‚ùå Missing OpenAI LLM. Install: pip install llama-index-llms-openai")

class RAGBot:
    """RAG Chatbot with document search and function calling"""
    
    def __init__(
        self,
        pinecone_api_key: str,
        openai_api_key: str,
        index_name: str = "attacker2",
        model: str = "gpt-4.1-mini"
    ):
        """Initialize RAG bot with Pinecone, OpenAI, and MCP server"""
        
        # Set environment variables
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        # Initialize MCP server for student data
        self.mcp_server = StudentDataMCPServer()
        self._mcp_initialized = False
        
        # Initialize Pinecone manager
        self.pinecone_manager = PineconeManager(
            api_key=pinecone_api_key,
            index_name=index_name
        )
        
        # Connect to existing index
        if not self.pinecone_manager.create_index():
            raise Exception("Failed to connect to Pinecone index")
        
        # Initialize LLM
        self.llm = OpenAI(
            model=model,
            temperature=0.7,
            max_tokens=400
        )
        
        # Load existing vector index
        if not self.pinecone_manager._load_existing_index():
            raise Exception("Failed to load existing vector index")
        
        # Create query engine
        self.query_engine = self._create_query_engine()
        
        # Chat memory
        self.memory = ChatMemoryBuffer.from_defaults(token_limit=20000)
        
        print(f"‚úÖ RAG Bot initialized with index: {index_name}")
        
    async def _ensure_mcp_connected(self):
        """Ensure MCP server is connected to database"""
        if not self._mcp_initialized:
            try:
                await self.mcp_server.connect_database()
                self._mcp_initialized = True
                print("‚úÖ MCP Academic Server connected")
            except Exception as e:
                print(f"‚ùå MCP connection failed: {e}")
                raise
    
    def _create_query_engine(self):
        """Create query engine for RAG"""
        try:
            # Create retriever
            retriever = VectorIndexRetriever(
                index=self.pinecone_manager.vector_index,
                similarity_top_k=5
            )
            
            # Create response synthesizer
            response_synthesizer = get_response_synthesizer(
                llm=self.llm,
                text_qa_template=self._get_qa_template(),
                refine_template=self._get_refine_template()
            )
            
            # Create query engine
            query_engine = RetrieverQueryEngine(
                retriever=retriever,
                response_synthesizer=response_synthesizer,
                node_postprocessors=[
                    SimilarityPostprocessor(similarity_cutoff=0.3)
                ]
            )
            
            return query_engine
            
        except Exception as e:
            print(f"‚ùå Error creating query engine: {e}")
            return None
    
    def _get_qa_template(self):
        """Get QA template for RAG responses"""
        from llama_index.core.prompts import PromptTemplate
        
        qa_template = PromptTemplate(
            "B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ t√≠n d·ª•ng sinh vi√™n. "
            "D·ª±a tr√™n th√¥ng tin sau ƒë√¢y: {context_str} "
            "H√£y tr·∫£ l·ªùi c√¢u h·ªèi: {query_str} "
            "Quy t·∫Øc tr·∫£ l·ªùi: Tr·∫£ l·ªùi b·∫±ng 1 ƒëo·∫°n vƒÉn li·ªÅn m·∫°ch, kh√¥ng xu·ªëng d√≤ng, "
            "kh√¥ng d·∫•u g·∫°ch ƒë·∫ßu d√≤ng, kh√¥ng markdown, ch·ªâ vƒÉn b·∫£n thu·∫ßn t√∫y. "
            "N·∫øu kh√¥ng c√≥ th√¥ng tin th√¨ n√≥i t√¥i kh√¥ng bi·∫øt "
            "Tr·∫£ l·ªùi:"
        )
        
        return qa_template
    
    def _get_refine_template(self):
        """Get refine template for multi-document answers"""
        from llama_index.core.prompts import PromptTemplate
        
        refine_template = PromptTemplate(
            "C√¢u h·ªèi: {query_str} "
            "Th√¥ng tin b·ªï sung: {context_msg} "
            "C√¢u tr·∫£ l·ªùi hi·ªán t·∫°i: {existing_answer} "
            "H√£y c·∫£i thi·ªán c√¢u tr·∫£ l·ªùi th√†nh 1 ƒëo·∫°n vƒÉn li·ªÅn m·∫°ch, kh√¥ng xu·ªëng d√≤ng, kh√¥ng markdown:"
        )
        
        return refine_template
    
    async def chat(self, message: str, citizen_id: Optional[str] = None) -> Dict[str, Any]:
        """Smart chat method using LLM to decide response strategy with conversation memory"""
        
        start_time = time.time()
        
        try:
            # Add user message to memory
            from llama_index.core.llms import ChatMessage
            user_message = ChatMessage(role="user", content=message)
            self.memory.put(user_message)
            
            # Get conversation context for better classification
            conversation_history = self._get_recent_conversation_context()
            
            # Use LLM to classify the question and decide response strategy
            response_strategy = await self._classify_question(message, citizen_id, conversation_history)
            
            response_dict = {}
            
            if response_strategy == "direct_answer":
                # LLM can answer directly without needing documents or personal data
                response_dict = await self._handle_direct_response(message, start_time, conversation_history)
            elif response_strategy == "call_data_db":
                # Database questions - get data from MCP server
                response_dict = await self._handle_database_data(message, citizen_id, start_time, conversation_history)
            elif response_strategy == "personal":
                # Personal questions - provide general guidance since no personal data available
                response_dict = await self._handle_personal_guidance(message, start_time, conversation_history)
            elif response_strategy == "rag_search":
                # Need to search documents for specific information
                response_dict = await self._handle_rag_query(message, start_time, conversation_history)
            else:
                # Default to RAG if unsure
                response_dict = await self._handle_rag_query(message, start_time, conversation_history)
            
            # Add assistant response to memory
            assistant_message = ChatMessage(role="assistant", content=response_dict["response"])
            self.memory.put(assistant_message)
            
            # Add conversation info to response
            response_dict["conversation_id"] = id(self.memory)
            response_dict["memory_tokens"] = len(str(self.memory.get()))
            
            return response_dict
                
        except Exception as e:
            error_response = {
                "response": f"Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}",
                "source": "error",
                "processing_time": time.time() - start_time,
                "error": str(e)
            }
            
            # Still add to memory for debugging
            try:
                from llama_index.core.llms import ChatMessage
                error_message = ChatMessage(role="assistant", content=error_response["response"])
                self.memory.put(error_message)
            except:
                pass
                
            return error_response
    
    def _get_recent_conversation_context(self) -> str:
        """Get recent conversation context for better understanding"""
        try:
            # Get recent messages from memory
            chat_history = self.memory.get()
            
            if not chat_history:
                return "Kh√¥ng c√≥ l·ªãch s·ª≠ tr√≤ chuy·ªán."
            
            # Format recent conversation (last 4 messages)
            recent_messages = chat_history[-6:] if len(chat_history) > 4 else chat_history
            context_parts = []
            
            for msg in recent_messages:
                role = "Ng∆∞·ªùi d√πng" if msg.role == "user" else "Tr·ª£ l√Ω"
                context_parts.append(f"{role}: {msg.content[:100]}")
            
            return "L·ªãch s·ª≠ tr√≤ chuy·ªán g·∫ßn ƒë√¢y:\n" + "\n".join(context_parts)
            
        except Exception as e:
            print(f"‚ùå Error getting conversation context: {e}")
            return "Kh√¥ng th·ªÉ l·∫•y l·ªãch s·ª≠ tr√≤ chuy·ªán."
    
    def clear_memory(self):
        """Clear conversation memory"""
        try:
            self.memory.reset()
            print("‚úÖ Conversation memory cleared")
        except Exception as e:
            print(f"‚ùå Error clearing memory: {e}")
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """Get conversation summary and statistics"""
        try:
            chat_history = self.memory.get()
            
            if not chat_history:
                return {
                    "message_count": 0,
                    "memory_tokens": 0,
                    "status": "empty"
                }
            
            user_messages = [msg for msg in chat_history if msg.role == "user"]
            assistant_messages = [msg for msg in chat_history if msg.role == "assistant"]
            
            return {
                "message_count": len(chat_history),
                "user_messages": len(user_messages),
                "assistant_messages": len(assistant_messages),
                "memory_tokens": len(str(chat_history)),
                "recent_topics": [msg.content[:50] + "..." for msg in user_messages[-3:]],
                "status": "active"
            }
            
        except Exception as e:
            return {
                "error": str(e),
                "status": "error"
            }
    
    async def _handle_personal_guidance(self, message: str, start_time: float, conversation_history: str = "") -> Dict[str, Any]:
        """Handle personal questions by providing general guidance"""
        
        try:
            personal_prompt = f"""
B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ t√≠n d·ª•ng sinh vi√™n c·ªßa Student Credit.
Ng∆∞·ªùi d√πng h·ªèi m·ªôt c√¢u h·ªèi c√° nh√¢n: "{message}"

L·ªãch s·ª≠ tr√≤ chuy·ªán:
{conversation_history}

H√£y tr·∫£ l·ªùi m·ªôt c√°ch h·ªØu √≠ch v√† h∆∞·ªõng d·∫´n ng∆∞·ªùi d√πng:
- ƒê∆∞a ra th√¥ng tin t·ªïng quan h·ªØu √≠ch v·ªÅ ch·ªß ƒë·ªÅ h·ªç h·ªèi
- Gi·∫£i th√≠ch c√°c y·∫øu t·ªë chung ·∫£nh h∆∞·ªüng ƒë·∫øn v·∫•n ƒë·ªÅ n√†y
- H∆∞·ªõng d·∫´n c√°ch t·ª± ƒë√°nh gi√° ho·∫∑c chu·∫©n b·ªã
- Tham kh·∫£o l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë·ªÉ cung c·∫•p c√¢u tr·∫£ l·ªùi ph√π h·ª£p
- Tr·∫£ l·ªùi th√¢n thi·ªán, h·ªØu √≠ch

Quy t·∫Øc tr·∫£ l·ªùi:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát t·ª± nhi√™n
- ƒê∆∞a ra th√¥ng tin h·ªØu √≠ch v√† th·ª±c ti·ªÖn
- Kh√¥ng n√≥i "t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi"
- 3-4 c√¢u ng·∫Øn g·ªçn

Tr·∫£ l·ªùi:
"""
            
            response = await self.llm.acomplete(personal_prompt)
            response_text = str(response).strip()
            
            return {
                "response": response_text,
                "source": "personal_general",
                "processing_time": round(time.time() - start_time, 3),
                "requires_login": False,
                "suggestion": "Th√¥ng tin t·ªïng quan v·ªÅ ch·ªß ƒë·ªÅ b·∫°n quan t√¢m"
            }
            
        except Exception as e:
            print(f"‚ùå Personal query error: {e}")
            return {
                "response": "T√¥i c√≥ th·ªÉ cung c·∫•p th√¥ng tin t·ªïng quan v·ªÅ vay v·ªën sinh vi√™n. ƒê·ªÉ c√≥ l·ªùi khuy√™n c·ª• th·ªÉ h∆°n, b·∫°n c√≥ th·ªÉ chia s·∫ª th√™m v·ªÅ t√¨nh hu·ªëng c·ªßa m√¨nh. B·∫°n c√≥ mu·ªën bi·∫øt v·ªÅ ƒëi·ªÅu ki·ªán vay, quy tr√¨nh, hay l√£i su·∫•t kh√¥ng?",
                "source": "personal_fallback",
                "processing_time": round(time.time() - start_time, 3),
                "requires_login": False,
                "error": str(e)
            }
    
    async def _handle_database_data(self, message: str, citizen_id: Optional[str], start_time: float, conversation_history: str = "") -> Dict[str, Any]:
        """Handle database data questions using MCP server"""
        
        if not citizen_id:
            return {
                "response": "ƒê·ªÉ truy c·∫≠p th√¥ng tin c√° nh√¢n, b·∫°n c·∫ßn ƒëƒÉng nh·∫≠p v√†o h·ªá th·ªëng tr∆∞·ªõc. Vui l√≤ng ƒëƒÉng nh·∫≠p ƒë·ªÉ t√¥i c√≥ th·ªÉ cung c·∫•p th√¥ng tin ch√≠nh x√°c v·ªÅ d·ªØ li·ªáu c√° nh√¢n c·ªßa b·∫°n.",
                "source": "database_login_required",
                "processing_time": round(time.time() - start_time, 3),
                "requires_login": True
            }
        
        try:
            # Ensure MCP server is connected
            await self._ensure_mcp_connected()
            
            # Check if this is a pure greeting message (not mixed with questions)
            greeting_keywords = ["xin ch√†o", "hello", "ch√†o", "hi", "hey"]
            academic_keywords = ["gpa", "ƒëi·ªÉm", "t√≠n ch·ªâ", "h·ªçc b·ªïng", "th√†nh t√≠ch", "c√¢u l·∫°c b·ªô", "ho·∫°t ƒë·ªông", "l√£nh ƒë·∫°o", "nƒÉm h·ªçc", "h·ªçc k·ª≥"]
            
            # Check if it's a pure greeting (no academic keywords)
            is_pure_greeting = (
                any(keyword in message.lower() for keyword in greeting_keywords) and
                not any(keyword in message.lower() for keyword in academic_keywords) and
                len(message.strip()) < 20  # Short message, likely just greeting
            )
            
            if is_pure_greeting:
                # Get user data for greeting
                user_data = await self.mcp_server.get_user_data(citizen_id)
                
                if "error" not in user_data:
                    user_name = user_data.get('name', 'b·∫°n')
                    return {
                        "response": f"Xin ch√†o {user_name}! T√¥i l√† tr·ª£ l√Ω AI c·ªßa Student Credit. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m hi·ªÉu v·ªÅ th√¥ng tin h·ªçc t·∫≠p, h·ªì s∆° sinh vi√™n v√† c√°c d·ªãch v·ª• vay v·ªën. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨ h√¥m nay?",
                        "source": "personalized_greeting",
                        "processing_time": round(time.time() - start_time, 3),
                        "user_data": user_data,
                        "citizen_id": citizen_id
                    }
                else:
                    # Fallback if user data not found
                    return {
                        "response": "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω AI c·ªßa Student Credit. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m hi·ªÉu v·ªÅ th√¥ng tin h·ªçc t·∫≠p, h·ªì s∆° sinh vi√™n v√† c√°c d·ªãch v·ª• vay v·ªën. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨ h√¥m nay?",
                        "source": "greeting_fallback",
                        "processing_time": round(time.time() - start_time, 3),
                        "citizen_id": citizen_id
                    }
            
            # Get academic data from MCP server for other questions
            academic_data = await self.mcp_server.get_academic_data(citizen_id)
            
            if "error" in academic_data:
                return {
                    "response": f"Xin l·ªói, t√¥i kh√¥ng th·ªÉ t√¨m th·∫•y th√¥ng tin d·ªØ li·ªáu cho t√†i kho·∫£n c·ªßa b·∫°n. Vui l√≤ng ki·ªÉm tra l·∫°i ho·∫∑c li√™n h·ªá h·ªó tr·ª£ ƒë·ªÉ ƒë∆∞·ª£c tr·ª£ gi√∫p.",
                    "source": "database_not_found",
                    "processing_time": round(time.time() - start_time, 3),
                    "error": academic_data["error"]
                }
            
            # Format academic data for natural language response
            academic_context = f"""
Th√¥ng tin h·ªçc t·∫≠p c·ªßa sinh vi√™n (ID: {academic_data['student_id']}):

üìä TH√ÄNH T√çCH H·ªåC T·∫¨P:
‚Ä¢ GPA t·ªïng: {academic_data['academic_performance']['gpa']}/4.0
‚Ä¢ GPA h·ªçc k·ª≥ hi·ªán t·∫°i: {academic_data['academic_performance']['current_gpa']}/4.0
‚Ä¢ T·ªïng t√≠n ch·ªâ ƒë√£ ho√†n th√†nh: {academic_data['academic_performance']['total_credits_earned']}
‚Ä¢ S·ªë m√¥n thi r·ªõt: {academic_data['academic_performance']['failed_course_count']}

üèÜ TH√ÄNH T·ª∞U & H·ªåC B·ªîNG:
‚Ä¢ S·ªë gi·∫£i th∆∞·ªüng/th√†nh t√≠ch: {academic_data['achievements']['achievement_award_count']}
‚Ä¢ C√≥ h·ªçc b·ªïng: {'C√≥' if academic_data['achievements']['has_scholarship'] else 'Kh√¥ng'}
‚Ä¢ S·ªë l∆∞·ª£ng h·ªçc b·ªïng: {academic_data['achievements']['scholarship_count']}

üéØ HO·∫†T ƒê·ªòNG & L√ÉNH ƒê·∫†O:
‚Ä¢ C√¢u l·∫°c b·ªô: {academic_data['activities']['club']}
‚Ä¢ S·ªë ho·∫°t ƒë·ªông ngo·∫°i kh√≥a: {academic_data['activities']['extracurricular_activity_count']}
‚Ä¢ C√≥ vai tr√≤ l√£nh ƒë·∫°o: {'C√≥' if academic_data['activities']['has_leadership_role'] else 'Kh√¥ng'}

üìö T√åNH TR·∫†NG HI·ªÜN T·∫†I:
‚Ä¢ NƒÉm h·ªçc: {academic_data['current_status']['study_year']}
‚Ä¢ H·ªçc k·ª≥: {academic_data['current_status']['term']}
‚Ä¢ Tr·∫°ng th√°i x√°c th·ª±c: {'ƒê√£ x√°c th·ª±c' if academic_data['current_status']['verified'] else 'Ch∆∞a x√°c th·ª±c'}
"""

            # Generate natural language response based on question and data
            academic_prompt = f"""
B·∫°n l√† tr·ª£ l√Ω AI c·ªßa Student Credit, chuy√™n t∆∞ v·∫•n v·ªÅ t√≠n d·ª•ng sinh vi√™n.
Ng∆∞·ªùi d√πng h·ªèi: "{message}"

L·ªãch s·ª≠ tr√≤ chuy·ªán:
{conversation_history}

D·ª±a v√†o th√¥ng tin h·ªçc t·∫≠p sau c·ªßa sinh vi√™n:
{academic_context}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch t·ª± nhi√™n, th√¢n thi·ªán v√† h·ªØu √≠ch:
- Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi ƒë∆∞·ª£c h·ªèi
- S·ª≠ d·ª•ng th√¥ng tin c·ª• th·ªÉ t·ª´ h·ªì s∆° h·ªçc t·∫≠p
- Tham kh·∫£o l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë·ªÉ tr√°nh l·∫∑p l·∫°i th√¥ng tin
- C√≥ th·ªÉ ƒë∆∞a ra nh·∫≠n x√©t ho·∫∑c l·ªùi khuy√™n ph√π h·ª£p
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát t·ª± nhi√™n
- Ng·∫Øn g·ªçn, 2-3 c√¢u

Tr·∫£ l·ªùi:
"""
            
            response = await self.llm.acomplete(academic_prompt)
            response_text = str(response).strip()
            
            return {
                "response": response_text,
                "source": "database_data",
                "processing_time": round(time.time() - start_time, 3),
                "data": academic_data,
                "citizen_id": citizen_id
            }
            
        except Exception as e:
            print(f"‚ùå Database data error: {e}")
            return {
                "response": "Xin l·ªói, t√¥i g·∫∑p l·ªói khi truy c·∫≠p th√¥ng tin d·ªØ li·ªáu c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá h·ªó tr·ª£.",
                "source": "database_error",
                "processing_time": round(time.time() - start_time, 3),
                "error": str(e)
            }

    async def _classify_question(self, message: str, citizen_id: Optional[str] = None, conversation_history: str = "") -> str:
        """Use LLM to classify question and decide response strategy with conversation context"""
        
        context_info = f"Ng∆∞·ªùi d√πng {'c√≥' if citizen_id else 'kh√¥ng c√≥'} th√¥ng tin ƒë·ªãnh danh (citizen_id)."
        
        classification_prompt = f"""
B·∫°n l√† m·ªôt AI classifier cho h·ªá th·ªëng chatbot t∆∞ v·∫•n vay v·ªën sinh vi√™n.
H√£y ph√¢n lo·∫°i c√¢u h·ªèi sau v√† quy·∫øt ƒë·ªãnh chi·∫øn l∆∞·ª£c tr·∫£ l·ªùi t·ªët nh·∫•t.

C√¢u h·ªèi hi·ªán t·∫°i: "{message}"
Th√¥ng tin ng·ªØ c·∫£nh: {context_info}

L·ªãch s·ª≠ tr√≤ chuy·ªán:
{conversation_history}

QUY T·∫ÆC PH√ÇN LO·∫†I QUAN TR·ªåNG (d·ª±a tr√™n context v√† l·ªãch s·ª≠):
- N·∫øu c√¢u h·ªèi CH·ªà l√† ch√†o h·ªèi thu·∫ßn t√∫y (kh√¥ng c√≥ t·ª´ kh√≥a h·ªçc t·∫≠p) + user ƒë√£ ƒëƒÉng nh·∫≠p ‚Üí call_data_db
- N·∫øu c√¢u h·ªèi c√≥ t·ª´ kh√≥a h·ªçc t·∫≠p (gpa, ƒëi·ªÉm, t√≠n ch·ªâ, etc.) ‚Üí call_data_db
- N·∫øu c√¢u h·ªèi ch√†o h·ªèi + user ch∆∞a ƒëƒÉng nh·∫≠p ‚Üí direct_answer
- N·∫øu c√¢u h·ªèi chung v·ªÅ vay v·ªën ho·∫∑c follow-up t·ª´ conversation tr∆∞·ªõc ‚Üí rag_search
- N·∫øu c√¢u h·ªèi ti·∫øp theo li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ ƒë√£ th·∫£o lu·∫≠n ‚Üí gi·ªØ nguy√™n strategy t·ª´ context

PH√ÇN LO·∫†I:
1. "direct_answer" - Ch√†o h·ªèi khi ch∆∞a ƒëƒÉng nh·∫≠p, c·∫£m ∆°n, c√¢u h·ªèi chung kh√¥ng c·∫ßn th√¥ng tin c√° nh√¢n
2. "call_data_db" - Ch√†o h·ªèi khi ƒë√£ ƒëƒÉng nh·∫≠p ho·∫∑c c√¢u h·ªèi v·ªÅ d·ªØ li·ªáu c√° nh√¢n (academic, profile, etc.)
3. "personal" - C√¢u h·ªèi v·ªÅ th√¥ng tin c√° nh√¢n kh√°c kh√¥ng c√≥ trong database
4. "rag_search" - C√¢u h·ªèi c·∫ßn th√¥ng tin t·ª´ t√†i li·ªáu, quy ƒë·ªãnh chung v·ªÅ vay v·ªën

V√ç D·ª§ C·ª§ TH·ªÇ:

DIRECT_ANSWER (ch∆∞a ƒëƒÉng nh·∫≠p):
- "Xin ch√†o" (kh√¥ng c√≥ citizen_id) ‚Üí direct_answer
- "C·∫£m ∆°n b·∫°n" ‚Üí direct_answer
- "B·∫°n c√≥ th·ªÉ gi√∫p g√¨?" ‚Üí direct_answer

CALL_DATA_DB (c·∫ßn d·ªØ li·ªáu t·ª´ database):
- "Xin ch√†o" (c√≥ citizen_id) ‚Üí call_data_db (l·∫•y t√™n ƒë·ªÉ ch√†o)
- "Hello" (c√≥ citizen_id) ‚Üí call_data_db (l·∫•y t√™n ƒë·ªÉ ch√†o)
- "GPA c·ªßa t√¥i l√† bao nhi·ªÅu?" ‚Üí call_data_db
- "ƒëi·ªÉm s·ªë c·ªßa t√¥i" ‚Üí call_data_db
- "t√≠n ch·ªâ t√¥i ƒë√£ h·ªçc" ‚Üí call_data_db
- "h·ªçc b·ªïng c·ªßa t√¥i" ‚Üí call_data_db
- "th√†nh t√≠ch h·ªçc t·∫≠p" ‚Üí call_data_db
- "c√¢u l·∫°c b·ªô t√¥i tham gia" ‚Üí call_data_db

RAG_SEARCH (t√†i li·ªáu chung):
- "Quy tr√¨nh vay v·ªën nh∆∞ th·∫ø n√†o?" ‚Üí rag_search
- "L√£i su·∫•t vay sinh vi√™n" ‚Üí rag_search
- "ƒêi·ªÅu ki·ªán vay v·ªën" ‚Üí rag_search
- "Gi·∫•y t·ªù c·∫ßn thi·∫øt" ‚Üí rag_search
- Follow-up questions v·ªÅ vay v·ªën ‚Üí rag_search

PERSONAL (th√¥ng tin c√° nh√¢n kh√°c):
- "T√¥i c√≥ th·ªÉ vay bao nhi·ªÅu ti·ªÅn?" ‚Üí personal
- "H·ªì s∆° vay c·ªßa t√¥i" ‚Üí personal

CH·ªà TR·∫¢ L·ªúI M·ªòT T·ª™: direct_answer, call_data_db, personal, ho·∫∑c rag_search
"""
        
        try:
            # Use a fast, lightweight call to classify
            try:
                from llama_index.llms.openai import OpenAI
            except ImportError:
                from llama_index_llms_openai import OpenAI
                
            classifier_llm = OpenAI(model="gpt-4.1-mini", temperature=0, max_tokens=50)
            
            response = await classifier_llm.acomplete(classification_prompt)
            result = str(response).strip().lower()
            
            if "direct_answer" in result:
                return "direct_answer"
            elif "call_data_db" in result:
                return "call_data_db"
            elif "personal" in result:
                return "personal"
            elif "rag_search" in result:
                return "rag_search"
            else:
                # If LLM response is unclear, ask it again with simpler prompt
                simple_prompt = f"""
Classify this question into one category: direct_answer, call_data_db, personal, or rag_search

Question: "{message}"
Context: User {'has' if citizen_id else 'does not have'} login info.

Rules:
- Greetings with login ‚Üí call_data_db
- Academic questions ‚Üí call_data_db  
- General questions ‚Üí direct_answer
- Document questions ‚Üí rag_search

Answer with one word only:"""
                
                fallback_response = await classifier_llm.acomplete(simple_prompt)
                fallback_result = str(fallback_response).strip().lower()
                
                if "call_data_db" in fallback_result:
                    return "call_data_db"
                elif "direct_answer" in fallback_result:
                    return "direct_answer"
                elif "personal" in fallback_result:
                    return "personal"
                else:
                    return "rag_search"
                
        except Exception as e:
            print(f"‚ùå Classification error: {e}")
            # Only use rule-based as absolute last resort when LLM fails
            database_keywords = ["gpa", "ƒëi·ªÉm", "t√≠n ch·ªâ", "h·ªçc b·ªïng", "th√†nh t√≠ch", "c√¢u l·∫°c b·ªô", "ho·∫°t ƒë·ªông", "l√£nh ƒë·∫°o", "nƒÉm h·ªçc", "h·ªçc k·ª≥"]
            
            if any(keyword in message.lower() for keyword in database_keywords):
                return "call_data_db"
            else:
                return "direct_answer"  # Safe fallback
    
    async def _handle_direct_response(self, message: str, start_time: float, conversation_history: str = "") -> Dict[str, Any]:
        """Handle direct LLM responses for general questions"""
        
        try:
            direct_prompt = f"""
B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ t√≠n d·ª•ng sinh vi√™n c·ªßa Student Credit.
H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau m·ªôt c√°ch t·ª± nhi√™n, th√¢n thi·ªán v√† h·ªØu √≠ch.

C√¢u h·ªèi: {message}

L·ªãch s·ª≠ tr√≤ chuy·ªán:
{conversation_history}

Quy t·∫Øc tr·∫£ l·ªùi:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát t·ª± nhi√™n, th√¢n thi·ªán
- N·∫øu l√† ch√†o h·ªèi, h√£y gi·ªõi thi·ªáu b·∫£n th√¢n v√† d·ªãch v·ª•
- N·∫øu l√† c√¢u h·ªèi chung v·ªÅ vay v·ªën, ƒë∆∞a ra th√¥ng tin t·ªïng quan h·ªØu √≠ch
- Tham kh·∫£o l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë·ªÉ tr√°nh l·∫∑p l·∫°i th√¥ng tin
- Kh√¥ng c·∫ßn t√¨m ki·∫øm t√†i li·ªáu c·ª• th·ªÉ, d·ª±a v√†o ki·∫øn th·ª©c chung
- Khuy·∫øn kh√≠ch ng∆∞·ªùi d√πng h·ªèi th√™m n·∫øu c·∫ßn th√¥ng tin chi ti·∫øt
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch (2-3 c√¢u)

Tr·∫£ l·ªùi:
"""
            
            # Use the main LLM for direct response
            response = await self.llm.acomplete(direct_prompt)
            response_text = str(response).strip()
            
            return {
                "response": response_text,
                "source": "direct_llm",
                "processing_time": round(time.time() - start_time, 3)
            }
            
        except Exception as e:
            print(f"‚ùå Direct response error: {e}")
            return {
                "response": "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω AI c·ªßa Student Credit. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªÅ c√°c th√¥ng tin vay v·ªën sinh vi√™n. B·∫°n c√≥ c√¢u h·ªèi g√¨ kh√¥ng?",
                "source": "fallback",
                "processing_time": round(time.time() - start_time, 3),
                "error": str(e)
            }
    
    async def _handle_rag_query(self, message: str, start_time: float, conversation_history: str = "") -> Dict[str, Any]:
        """Handle RAG document search queries"""
        
        if not self.query_engine:
            return {
                "response": "H·ªá th·ªëng t√¨m ki·∫øm t√†i li·ªáu ch∆∞a s·∫µn s√†ng. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "source": "error",
                "processing_time": time.time() - start_time
            }
        
        try:
            # Enhance query with conversation context
            enhanced_query = message
            if conversation_history and "L·ªãch s·ª≠ tr√≤ chuy·ªán g·∫ßn ƒë√¢y:" in conversation_history:
                enhanced_query = f"{message} (Ng·ªØ c·∫£nh: {conversation_history[-200:]})"
            
            # Query the knowledge base
            response = self.query_engine.query(enhanced_query)
            
            # Extract source information
            sources = []
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes[:3]:  # Top 3 sources
                    sources.append({
                        "text_preview": node.text[:100] + "...",
                        "score": getattr(node, 'score', 0),
                        "metadata": node.metadata
                    })
            
            return {
                "response": str(response),
                "source": "knowledge_base",
                "sources": sources,
                "processing_time": time.time() - start_time,
                "query_stats": {
                    "retrieved_documents": len(sources),
                    "has_answer": len(str(response)) > 50,
                    "enhanced_with_context": len(conversation_history) > 0
                }
            }
            
        except Exception as e:
            return {
                "response": f"Kh√¥ng th·ªÉ t√¨m ki·∫øm trong t√†i li·ªáu: {str(e)}",
                "source": "error",
                "processing_time": time.time() - start_time,
                "error": str(e)
            }
    
    def get_stats(self) -> Dict[str, Any]:
        """Get RAG bot statistics with memory info"""
        
        try:
            pinecone_stats = self.pinecone_manager.get_index_stats()
            conversation_stats = self.get_conversation_summary()
            
            return {
                "status": "ready",
                "pinecone_stats": pinecone_stats,
                "conversation_memory": conversation_stats,
                "features": {
                    "document_search": True,
                    "function_calling": True,  # MCP server integration
                    "personal_context": True,  # Database integration with memory
                    "conversation_memory": True,
                    "smart_routing": True  # LLM-based classification with context
                },
                "model": "gpt-4.1-mini",
                "embedding_model": "text-embedding-3-large",
                "response_strategies": ["direct_answer", "call_data_db", "personal", "rag_search"],
                "memory_system": "ChatMemoryBuffer with conversation context"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }

# Helper functions for API
def create_rag_bot() -> RAGBot:
    """Create RAG bot instance with environment variables"""
    
    load_dotenv()
    
    openai_key = os.getenv("OPENAI_API_KEY")
    pinecone_key = os.getenv("PINECONE_API_KEY")
    
    if not openai_key or not pinecone_key:
        raise Exception("Missing OPENAI_API_KEY or PINECONE_API_KEY in environment")
    
    return RAGBot(
        pinecone_api_key=pinecone_key,
        openai_api_key=openai_key
    )

# Global bot instance (singleton)
_bot_instance = None

def get_rag_bot() -> RAGBot:
    """Get or create global RAG bot instance"""
    global _bot_instance
    
    if _bot_instance is None:
        _bot_instance = create_rag_bot()
    
    return _bot_instance
