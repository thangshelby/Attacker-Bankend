"""
RAG Chatbot with Function Calling
Combines document search (RAG) with database function calling via MCP server
"""
import os
import time
import asyncio
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# Import existing components
from app.botagent.vectordb import PineconeManager
from app.botagent.mcp_server import AcademicMCPServer

# LlamaIndex imports with fallback
try:
    from llama_index.core.chat_engine import CondensePlusContextChatEngine
    from llama_index.core.memory import ChatMemoryBuffer
    from llama_index.core import get_response_synthesizer
    from llama_index.core.retrievers import VectorIndexRetriever
    from llama_index.core.query_engine import RetrieverQueryEngine
    from llama_index.core.postprocessor import SimilarityPostprocessor
except ImportError as e:
    print(f"âŒ LlamaIndex import error: {e}")
    print("   Install: pip install llama-index")

try:
    from llama_index.llms.openai import OpenAI
except ImportError:
    try:
        from llama_index_llms_openai import OpenAI
    except ImportError:
        print("âŒ Missing OpenAI LLM. Install: pip install llama-index-llms-openai")

class RAGBot:
    """RAG Chatbot with document search and function calling"""
    
    def __init__(
        self,
        pinecone_api_key: str,
        openai_api_key: str,
        index_name: str = "attacker2",
        model: str = "gpt-4.1-mini"
    ):
        """Initialize RAG bot with Pinecone, OpenAI, and MCP server"""
        
        # Set environment variables
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        # Initialize MCP server for academic data
        self.mcp_server = AcademicMCPServer()
        self._mcp_initialized = False
        
        # Initialize Pinecone manager
        self.pinecone_manager = PineconeManager(
            api_key=pinecone_api_key,
            index_name=index_name
        )
        
        # Connect to existing index
        if not self.pinecone_manager.create_index():
            raise Exception("Failed to connect to Pinecone index")
        
        # Initialize LLM
        self.llm = OpenAI(
            model=model,
            temperature=0.7,
            max_tokens=400
        )
        
        # Load existing vector index
        if not self.pinecone_manager._load_existing_index():
            raise Exception("Failed to load existing vector index")
        
        # Create query engine
        self.query_engine = self._create_query_engine()
        
        # Chat memory
        self.memory = ChatMemoryBuffer.from_defaults(token_limit=20000)
        
        print(f"âœ… RAG Bot initialized with index: {index_name}")
        
    async def _ensure_mcp_connected(self):
        """Ensure MCP server is connected to database"""
        if not self._mcp_initialized:
            try:
                await self.mcp_server.connect_database()
                self._mcp_initialized = True
                print("âœ… MCP Academic Server connected")
            except Exception as e:
                print(f"âŒ MCP connection failed: {e}")
                raise
    
    def _create_query_engine(self):
        """Create query engine for RAG"""
        try:
            # Create retriever
            retriever = VectorIndexRetriever(
                index=self.pinecone_manager.vector_index,
                similarity_top_k=5
            )
            
            # Create response synthesizer
            response_synthesizer = get_response_synthesizer(
                llm=self.llm,
                text_qa_template=self._get_qa_template(),
                refine_template=self._get_refine_template()
            )
            
            # Create query engine
            query_engine = RetrieverQueryEngine(
                retriever=retriever,
                response_synthesizer=response_synthesizer,
                node_postprocessors=[
                    SimilarityPostprocessor(similarity_cutoff=0.3)
                ]
            )
            
            return query_engine
            
        except Exception as e:
            print(f"âŒ Error creating query engine: {e}")
            return None
    
    def _get_qa_template(self):
        """Get QA template for RAG responses"""
        from llama_index.core.prompts import PromptTemplate
        
        qa_template = PromptTemplate(
            "Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» tÃ­n dá»¥ng sinh viÃªn. "
            "Dá»±a trÃªn thÃ´ng tin sau Ä‘Ã¢y: {context_str} "
            "HÃ£y tráº£ lá»i cÃ¢u há»i: {query_str} "
            "Quy táº¯c tráº£ lá»i: Tráº£ lá»i báº±ng 1 Ä‘oáº¡n vÄƒn liá»n máº¡ch, khÃ´ng xuá»‘ng dÃ²ng, "
            "khÃ´ng dáº¥u gáº¡ch Ä‘áº§u dÃ²ng, khÃ´ng markdown, chá»‰ vÄƒn báº£n thuáº§n tÃºy. "
            "Náº¿u khÃ´ng cÃ³ thÃ´ng tin thÃ¬ nÃ³i tÃ´i khÃ´ng biáº¿t "
            "Tráº£ lá»i:"
        )
        
        return qa_template
    
    def _get_refine_template(self):
        """Get refine template for multi-document answers"""
        from llama_index.core.prompts import PromptTemplate
        
        refine_template = PromptTemplate(
            "CÃ¢u há»i: {query_str} "
            "ThÃ´ng tin bá»• sung: {context_msg} "
            "CÃ¢u tráº£ lá»i hiá»‡n táº¡i: {existing_answer} "
            "HÃ£y cáº£i thiá»‡n cÃ¢u tráº£ lá»i thÃ nh 1 Ä‘oáº¡n vÄƒn liá»n máº¡ch, khÃ´ng xuá»‘ng dÃ²ng, khÃ´ng markdown:"
        )
        
        return refine_template
    
    async def chat(self, message: str, citizen_id: Optional[str] = None) -> Dict[str, Any]:
        """Smart chat method using LLM to decide response strategy"""
        
        start_time = time.time()
        
        try:
            # Use LLM to classify the question and decide response strategy
            response_strategy = await self._classify_question(message, citizen_id)
            
            if response_strategy == "direct_answer":
                # LLM can answer directly without needing documents or personal data
                return await self._handle_direct_response(message, start_time)
            elif response_strategy == "call_data_db":
                # Database questions - get data from MCP server
                return await self._handle_database_data(message, citizen_id, start_time)
            elif response_strategy == "personal":
                # Personal questions - provide general guidance since no personal data available
                return await self._handle_personal_guidance(message, start_time)
            elif response_strategy == "rag_search":
                # Need to search documents for specific information
                return await self._handle_rag_query(message, start_time)
            else:
                # Default to RAG if unsure
                return await self._handle_rag_query(message, start_time)
                
        except Exception as e:
            return {
                "response": f"Xin lá»—i, tÃ´i gáº·p lá»—i khi xá»­ lÃ½ cÃ¢u há»i: {str(e)}",
                "source": "error",
                "processing_time": time.time() - start_time,
                "error": str(e)
            }
    
    async def _handle_personal_guidance(self, message: str, start_time: float) -> Dict[str, Any]:
        """Handle personal questions by providing general guidance"""
        
        try:
            personal_prompt = f"""
Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» tÃ­n dá»¥ng sinh viÃªn cá»§a Student Credit.
NgÆ°á»i dÃ¹ng há»i má»™t cÃ¢u há»i cÃ¡ nhÃ¢n: "{message}"

HÃ£y tráº£ lá»i má»™t cÃ¡ch há»¯u Ã­ch vÃ  hÆ°á»›ng dáº«n ngÆ°á»i dÃ¹ng:
- ÄÆ°a ra thÃ´ng tin tá»•ng quan há»¯u Ã­ch vá» chá»§ Ä‘á» há» há»i
- Giáº£i thÃ­ch cÃ¡c yáº¿u tá»‘ chung áº£nh hÆ°á»Ÿng Ä‘áº¿n váº¥n Ä‘á» nÃ y
- HÆ°á»›ng dáº«n cÃ¡ch tá»± Ä‘Ã¡nh giÃ¡ hoáº·c chuáº©n bá»‹
- Tráº£ lá»i thÃ¢n thiá»‡n, há»¯u Ã­ch

Quy táº¯c tráº£ lá»i:
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn
- ÄÆ°a ra thÃ´ng tin há»¯u Ã­ch vÃ  thá»±c tiá»…n
- KhÃ´ng nÃ³i "tÃ´i khÃ´ng thá»ƒ tráº£ lá»i"
- 3-4 cÃ¢u ngáº¯n gá»n

Tráº£ lá»i:
"""
            
            response = await self.llm.acomplete(personal_prompt)
            response_text = str(response).strip()
            
            return {
                "response": response_text,
                "source": "personal_general",
                "processing_time": round(time.time() - start_time, 3),
                "requires_login": False,
                "suggestion": "ThÃ´ng tin tá»•ng quan vá» chá»§ Ä‘á» báº¡n quan tÃ¢m"
            }
            
        except Exception as e:
            print(f"âŒ Personal query error: {e}")
            return {
                "response": "TÃ´i cÃ³ thá»ƒ cung cáº¥p thÃ´ng tin tá»•ng quan vá» vay vá»‘n sinh viÃªn. Äá»ƒ cÃ³ lá»i khuyÃªn cá»¥ thá»ƒ hÆ¡n, báº¡n cÃ³ thá»ƒ chia sáº» thÃªm vá» tÃ¬nh huá»‘ng cá»§a mÃ¬nh. Báº¡n cÃ³ muá»‘n biáº¿t vá» Ä‘iá»u kiá»‡n vay, quy trÃ¬nh, hay lÃ£i suáº¥t khÃ´ng?",
                "source": "personal_fallback",
                "processing_time": round(time.time() - start_time, 3),
                "requires_login": False,
                "error": str(e)
            }
    
    async def _handle_database_data(self, message: str, citizen_id: Optional[str], start_time: float) -> Dict[str, Any]:
        """Handle database data questions using MCP server"""
        
        if not citizen_id:
            return {
                "response": "Äá»ƒ truy cáº­p thÃ´ng tin cÃ¡ nhÃ¢n, báº¡n cáº§n Ä‘Äƒng nháº­p vÃ o há»‡ thá»‘ng trÆ°á»›c. Vui lÃ²ng Ä‘Äƒng nháº­p Ä‘á»ƒ tÃ´i cÃ³ thá»ƒ cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c vá» dá»¯ liá»‡u cÃ¡ nhÃ¢n cá»§a báº¡n.",
                "source": "database_login_required",
                "processing_time": round(time.time() - start_time, 3),
                "requires_login": True
            }
        
        try:
            # Ensure MCP server is connected
            await self._ensure_mcp_connected()
            
            # Get academic data from MCP server
            academic_data = await self.mcp_server.get_academic_data(citizen_id)
            
            if "error" in academic_data:
                return {
                    "response": f"Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tÃ¬m tháº¥y thÃ´ng tin dá»¯ liá»‡u cho tÃ i khoáº£n cá»§a báº¡n. Vui lÃ²ng kiá»ƒm tra láº¡i hoáº·c liÃªn há»‡ há»— trá»£ Ä‘á»ƒ Ä‘Æ°á»£c trá»£ giÃºp.",
                    "source": "database_not_found",
                    "processing_time": round(time.time() - start_time, 3),
                    "error": academic_data["error"]
                }
            
            # Format academic data for natural language response
            academic_context = f"""
ThÃ´ng tin há»c táº­p cá»§a sinh viÃªn (ID: {academic_data['student_id']}):

ðŸ“Š THÃ€NH TÃCH Há»ŒC Táº¬P:
â€¢ GPA tá»•ng: {academic_data['academic_performance']['gpa']}/4.0
â€¢ GPA há»c ká»³ hiá»‡n táº¡i: {academic_data['academic_performance']['current_gpa']}/4.0
â€¢ Tá»•ng tÃ­n chá»‰ Ä‘Ã£ hoÃ n thÃ nh: {academic_data['academic_performance']['total_credits_earned']}
â€¢ Sá»‘ mÃ´n thi rá»›t: {academic_data['academic_performance']['failed_course_count']}

ðŸ† THÃ€NH Tá»°U & Há»ŒC Bá»”NG:
â€¢ Sá»‘ giáº£i thÆ°á»Ÿng/thÃ nh tÃ­ch: {academic_data['achievements']['achievement_award_count']}
â€¢ CÃ³ há»c bá»•ng: {'CÃ³' if academic_data['achievements']['has_scholarship'] else 'KhÃ´ng'}
â€¢ Sá»‘ lÆ°á»£ng há»c bá»•ng: {academic_data['achievements']['scholarship_count']}

ðŸŽ¯ HOáº T Äá»˜NG & LÃƒNH Äáº O:
â€¢ CÃ¢u láº¡c bá»™: {academic_data['activities']['club']}
â€¢ Sá»‘ hoáº¡t Ä‘á»™ng ngoáº¡i khÃ³a: {academic_data['activities']['extracurricular_activity_count']}
â€¢ CÃ³ vai trÃ² lÃ£nh Ä‘áº¡o: {'CÃ³' if academic_data['activities']['has_leadership_role'] else 'KhÃ´ng'}

ðŸ“š TÃŒNH TRáº NG HIá»†N Táº I:
â€¢ NÄƒm há»c: {academic_data['current_status']['study_year']}
â€¢ Há»c ká»³: {academic_data['current_status']['term']}
â€¢ Tráº¡ng thÃ¡i xÃ¡c thá»±c: {'ÄÃ£ xÃ¡c thá»±c' if academic_data['current_status']['verified'] else 'ChÆ°a xÃ¡c thá»±c'}
"""

            # Generate natural language response based on question and data
            academic_prompt = f"""
Báº¡n lÃ  trá»£ lÃ½ AI cá»§a Student Credit, chuyÃªn tÆ° váº¥n vá» tÃ­n dá»¥ng sinh viÃªn.
NgÆ°á»i dÃ¹ng há»i: "{message}"

Dá»±a vÃ o thÃ´ng tin há»c táº­p sau cá»§a sinh viÃªn:
{academic_context}

HÃ£y tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch tá»± nhiÃªn, thÃ¢n thiá»‡n vÃ  há»¯u Ã­ch:
- Tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i Ä‘Æ°á»£c há»i
- Sá»­ dá»¥ng thÃ´ng tin cá»¥ thá»ƒ tá»« há»“ sÆ¡ há»c táº­p
- CÃ³ thá»ƒ Ä‘Æ°a ra nháº­n xÃ©t hoáº·c lá»i khuyÃªn phÃ¹ há»£p
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn
- Ngáº¯n gá»n, 2-3 cÃ¢u

Tráº£ lá»i:
"""
            
            response = await self.llm.acomplete(academic_prompt)
            response_text = str(response).strip()
            
            return {
                "response": response_text,
                "source": "database_data",
                "processing_time": round(time.time() - start_time, 3),
                "data": academic_data,
                "citizen_id": citizen_id
            }
            
        except Exception as e:
            print(f"âŒ Database data error: {e}")
            return {
                "response": "Xin lá»—i, tÃ´i gáº·p lá»—i khi truy cáº­p thÃ´ng tin dá»¯ liá»‡u cá»§a báº¡n. Vui lÃ²ng thá»­ láº¡i sau hoáº·c liÃªn há»‡ há»— trá»£.",
                "source": "database_error",
                "processing_time": round(time.time() - start_time, 3),
                "error": str(e)
            }

    async def _classify_question(self, message: str, citizen_id: Optional[str] = None) -> str:
        """Use LLM to classify question and decide response strategy"""
        
        context_info = f"NgÆ°á»i dÃ¹ng {'cÃ³' if citizen_id else 'khÃ´ng cÃ³'} thÃ´ng tin Ä‘á»‹nh danh (citizen_id)."
        
        classification_prompt = f"""
Báº¡n lÃ  má»™t AI classifier cho há»‡ thá»‘ng chatbot tÆ° váº¥n vay vá»‘n sinh viÃªn.
HÃ£y phÃ¢n loáº¡i cÃ¢u há»i sau vÃ  quyáº¿t Ä‘á»‹nh chiáº¿n lÆ°á»£c tráº£ lá»i tá»‘t nháº¥t.

CÃ¢u há»i: "{message}"
ThÃ´ng tin ngá»¯ cáº£nh: {context_info}

CÃ¡c loáº¡i cÃ¢u há»i vÃ  chiáº¿n lÆ°á»£c:
1. "direct_answer" - CÃ¢u há»i chÃ o há»i, cáº£m Æ¡n, hoáº·c cÃ¢u há»i chung khÃ´ng cáº§n thÃ´ng tin cÃ¡ nhÃ¢n
2. "call_data_db" - CÃ¢u há»i cáº§n truy váº¥n database Ä‘á»ƒ láº¥y dá»¯ liá»‡u cÃ¡ nhÃ¢n (academic, profile, etc.)
3. "personal" - CÃ¢u há»i vá» thÃ´ng tin cÃ¡ nhÃ¢n khÃ¡c cá»§a ngÆ°á»i dÃ¹ng (khÃ´ng cáº§n database)
4. "rag_search" - CÃ¢u há»i cáº§n thÃ´ng tin tá»« tÃ i liá»‡u, quy Ä‘á»‹nh chung vá» vay vá»‘n

VÃ­ dá»¥ phÃ¢n loáº¡i:

DIRECT_ANSWER:
- "Xin chÃ o" â†’ direct_answer
- "Cáº£m Æ¡n báº¡n" â†’ direct_answer  
- "Vay vá»‘n sinh viÃªn lÃ  gÃ¬?" â†’ direct_answer
- "Báº¡n cÃ³ thá»ƒ giÃºp gÃ¬?" â†’ direct_answer

CALL_DATA_DB (thÃ´ng tin cÃ¡ nhÃ¢n tá»« database):
- "Äiá»ƒm GPA cá»§a tÃ´i lÃ  bao nhiá»u?" â†’ call_data_db
- "TÃ´i cÃ³ bao nhiÃªu tÃ­n chá»‰?" â†’ call_data_db
- "ThÃ´ng tin há»c táº­p cá»§a tÃ´i nhÆ° tháº¿ nÃ o?" â†’ call_data_db
- "TÃ´i cÃ³ há»c bá»•ng khÃ´ng?" â†’ call_data_db
- "Káº¿t quáº£ há»c táº­p cá»§a tÃ´i ra sao?" â†’ call_data_db
- "TÃ´i tham gia cÃ¢u láº¡c bá»™ nÃ o?" â†’ call_data_db
- "Hoáº¡t Ä‘á»™ng ngoáº¡i khÃ³a cá»§a tÃ´i?" â†’ call_data_db
- "TÃ´i cÃ³ vai trÃ² lÃ£nh Ä‘áº¡o khÃ´ng?" â†’ call_data_db
- "NÄƒm há»c hiá»‡n táº¡i cá»§a tÃ´i?" â†’ call_data_db
- "Há»c ká»³ nÃ y cá»§a tÃ´i tháº¿ nÃ o?" â†’ call_data_db

PERSONAL (thÃ´ng tin cÃ¡ nhÃ¢n khÃ¡c):
- "TÃ´i cÃ³ thá»ƒ vay bao nhiá»u tiá»n?" â†’ personal
- "Há»“ sÆ¡ vay cá»§a tÃ´i nhÆ° tháº¿ nÃ o?" â†’ personal
- "TÃ¬nh tráº¡ng Ä‘Æ¡n vay cá»§a tÃ´i?" â†’ personal
- "Lá»‹ch sá»­ vay cá»§a tÃ´i ra sao?" â†’ personal
- "ThÃ´ng tin cÃ¡ nhÃ¢n cá»§a tÃ´i" â†’ personal

RAG_SEARCH (thÃ´ng tin chung tá»« tÃ i liá»‡u):
- "Quy trÃ¬nh vay vá»‘n nhÆ° tháº¿ nÃ o?" â†’ rag_search
- "LÃ£i suáº¥t vay sinh viÃªn hiá»‡n táº¡i?" â†’ rag_search
- "Äiá»u kiá»‡n vay vá»‘n lÃ  gÃ¬?" â†’ rag_search
- "Thá»i háº¡n vay tá»‘i Ä‘a bao lÃ¢u?" â†’ rag_search
- "Giáº¥y tá» cáº§n thiáº¿t Ä‘á»ƒ vay?" â†’ rag_search
- "System á»©ng dá»¥ng nhá»¯ng cÃ´ng nghá»‡ gÃ¬?" â†’ rag_search

Chá»‰ tráº£ lá»i má»™t tá»«: direct_answer, call_data_db, personal, hoáº·c rag_search
"""
        
        try:
            # Use a fast, lightweight call to classify
            try:
                from llama_index.llms.openai import OpenAI
            except ImportError:
                from llama_index_llms_openai import OpenAI
                
            classifier_llm = OpenAI(model="gpt-4.1-mini", temperature=0, max_tokens=50)
            
            response = await classifier_llm.acomplete(classification_prompt)
            result = str(response).strip().lower()
            
            if "direct_answer" in result:
                return "direct_answer"
            elif "call_data_db" in result:
                return "call_data_db"
            elif "personal" in result:
                return "personal"
            elif "rag_search" in result:
                return "rag_search"
            else:
                # Smart fallback logic
                database_keywords = ["gpa", "Ä‘iá»ƒm", "tÃ­n chá»‰", "há»c bá»•ng", "thÃ nh tÃ­ch", "cÃ¢u láº¡c bá»™", "hoáº¡t Ä‘á»™ng", "lÃ£nh Ä‘áº¡o", "nÄƒm há»c", "há»c ká»³"]
                personal_keywords = ["tÃ´i", "cá»§a tÃ´i", "há»“ sÆ¡", "Ä‘Æ¡n vay", "lá»‹ch sá»­ vay"]
                
                if any(keyword in message.lower() for keyword in database_keywords):
                    return "call_data_db"
                elif any(keyword in message.lower() for keyword in personal_keywords):
                    return "personal"
                else:
                    return "rag_search"  # Default to RAG for general info
                
        except Exception as e:
            print(f"âŒ Classification error: {e}")
            # Fallback classification based on keywords
            database_keywords = ["gpa", "Ä‘iá»ƒm", "tÃ­n chá»‰", "há»c bá»•ng", "thÃ nh tÃ­ch", "cÃ¢u láº¡c bá»™", "hoáº¡t Ä‘á»™ng", "lÃ£nh Ä‘áº¡o", "nÄƒm há»c", "há»c ká»³"]
            personal_keywords = ["tÃ´i", "cá»§a tÃ´i", "há»“ sÆ¡", "Ä‘Æ¡n vay", "thÃ´ng tin cÃ¡ nhÃ¢n", "lá»‹ch sá»­ vay"]
            greeting_keywords = ["xin chÃ o", "hello", "chÃ o", "cáº£m Æ¡n", "thank"]
            
            message_lower = message.lower()
            
            if any(keyword in message_lower for keyword in greeting_keywords):
                return "direct_answer"
            elif any(keyword in message_lower for keyword in database_keywords):
                return "call_data_db"
            elif any(keyword in message_lower for keyword in personal_keywords):
                return "personal"
            else:
                return "rag_search"
    
    async def _handle_direct_response(self, message: str, start_time: float) -> Dict[str, Any]:
        """Handle direct LLM responses for general questions"""
        
        try:
            direct_prompt = f"""
Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» tÃ­n dá»¥ng sinh viÃªn cá»§a Student Credit.
HÃ£y tráº£ lá»i cÃ¢u há»i sau má»™t cÃ¡ch tá»± nhiÃªn, thÃ¢n thiá»‡n vÃ  há»¯u Ã­ch.

CÃ¢u há»i: {message}

Quy táº¯c tráº£ lá»i:
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn, thÃ¢n thiá»‡n
- Náº¿u lÃ  chÃ o há»i, hÃ£y giá»›i thiá»‡u báº£n thÃ¢n vÃ  dá»‹ch vá»¥
- Náº¿u lÃ  cÃ¢u há»i chung vá» vay vá»‘n, Ä‘Æ°a ra thÃ´ng tin tá»•ng quan há»¯u Ã­ch
- KhÃ´ng cáº§n tÃ¬m kiáº¿m tÃ i liá»‡u cá»¥ thá»ƒ, dá»±a vÃ o kiáº¿n thá»©c chung
- Khuyáº¿n khÃ­ch ngÆ°á»i dÃ¹ng há»i thÃªm náº¿u cáº§n thÃ´ng tin chi tiáº¿t
- Tráº£ lá»i ngáº¯n gá»n, sÃºc tÃ­ch (2-3 cÃ¢u)

Tráº£ lá»i:
"""
            
            # Use the main LLM for direct response
            response = await self.llm.acomplete(direct_prompt)
            response_text = str(response).strip()
            
            return {
                "response": response_text,
                "source": "direct_llm",
                "processing_time": round(time.time() - start_time, 3)
            }
            
        except Exception as e:
            print(f"âŒ Direct response error: {e}")
            return {
                "response": "Xin chÃ o! TÃ´i lÃ  trá»£ lÃ½ AI cá»§a Student Credit. TÃ´i cÃ³ thá»ƒ giÃºp báº¡n vá» cÃ¡c thÃ´ng tin vay vá»‘n sinh viÃªn. Báº¡n cÃ³ cÃ¢u há»i gÃ¬ khÃ´ng?",
                "source": "fallback",
                "processing_time": round(time.time() - start_time, 3),
                "error": str(e)
            }
    
    async def _handle_rag_query(self, message: str, start_time: float) -> Dict[str, Any]:
        """Handle RAG document search queries"""
        
        if not self.query_engine:
            return {
                "response": "Há»‡ thá»‘ng tÃ¬m kiáº¿m tÃ i liá»‡u chÆ°a sáºµn sÃ ng. Vui lÃ²ng thá»­ láº¡i sau.",
                "source": "error",
                "processing_time": time.time() - start_time
            }
        
        try:
            # Query the knowledge base
            response = self.query_engine.query(message)
            
            # Extract source information
            sources = []
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes[:3]:  # Top 3 sources
                    sources.append({
                        "text_preview": node.text[:100] + "...",
                        "score": getattr(node, 'score', 0),
                        "metadata": node.metadata
                    })
            
            return {
                "response": str(response),
                "source": "knowledge_base",
                "sources": sources,
                "processing_time": time.time() - start_time,
                "query_stats": {
                    "retrieved_documents": len(sources),
                    "has_answer": len(str(response)) > 50
                }
            }
            
        except Exception as e:
            return {
                "response": f"KhÃ´ng thá»ƒ tÃ¬m kiáº¿m trong tÃ i liá»‡u: {str(e)}",
                "source": "error",
                "processing_time": time.time() - start_time,
                "error": str(e)
            }
    
    def get_stats(self) -> Dict[str, Any]:
        """Get RAG bot statistics"""
        
        try:
            pinecone_stats = self.pinecone_manager.get_index_stats()
            
            return {
                "status": "ready",
                "pinecone_stats": pinecone_stats,
                "features": {
                    "document_search": True,
                    "function_calling": False,  # No function calling
                    "personal_context": False,  # No personal data from database
                    "conversation_memory": True,
                    "smart_routing": True  # Auto-route to RAG/Direct based on question
                },
                "model": "gpt-4.1-mini",
                "embedding_model": "text-embedding-3-large",
                "response_strategies": ["direct_answer", "call_data_db", "personal", "rag_search"]
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }

# Helper functions for API
def create_rag_bot() -> RAGBot:
    """Create RAG bot instance with environment variables"""
    
    load_dotenv()
    
    openai_key = os.getenv("OPENAI_API_KEY")
    pinecone_key = os.getenv("PINECONE_API_KEY")
    
    if not openai_key or not pinecone_key:
        raise Exception("Missing OPENAI_API_KEY or PINECONE_API_KEY in environment")
    
    return RAGBot(
        pinecone_api_key=pinecone_key,
        openai_api_key=openai_key
    )

# Global bot instance (singleton)
_bot_instance = None

def get_rag_bot() -> RAGBot:
    """Get or create global RAG bot instance"""
    global _bot_instance
    
    if _bot_instance is None:
        _bot_instance = create_rag_bot()
    
    return _bot_instance
